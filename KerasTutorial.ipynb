{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tutorial\n",
    "(based on https://keras.io/getting-started/functional-api-guide/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we start, generate some random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=2)\n",
    "from keras.preprocessing.text import one_hot\n",
    "N = 100\n",
    "data = np.random.randn(N, 784)\n",
    "labels = np.eye(10)[np.zeros((N), dtype=int)]\n",
    "headline_data = np.random.randint(0, 10000, (N, 100))\n",
    "additional_data = np.random.randn(N, 5)\n",
    "labels2 = np.random.randint(0, 2, N)\n",
    "data_a = np.eye(256)[np.random.randint(0, 256, (N, 140))]\n",
    "data_b = np.eye(256)[np.random.randint(0, 256, (N, 140))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First example: fully connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "# this returns a tensor\n",
    "inputs = Input(shape=(784,))\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# this creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "model1 = Model(input=inputs, output=predictions)\n",
    "model1.compile(optimizer='rmsprop',\n",
    "               loss='categorical_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "model1.fit(data, labels, nb_epoch=100, verbose=0)  # starts training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## and evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "output_func = K.function([model1.layers[0].input, K.learning_phase()],\n",
    "                         [model1.layers[-1].output])\n",
    "\n",
    "# output with standard dropout approximation\n",
    "layer_output = output_func([data[:2], 0])[0]\n",
    "print('dropout approximation')\n",
    "print(layer_output)\n",
    "\n",
    "# output with MC dropout\n",
    "layer_outputs = []\n",
    "for _ in range(100):\n",
    "    layer_outputs += [output_func([data[:2], 1])[0]]\n",
    "    \n",
    "print('predictive mean')\n",
    "print(np.mean(layer_outputs, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Challenge 1</font>\n",
    "Adapt the code above to perform regression with tanh non-linearities, 3 layers, and 128 units in each layer. Calculate the predictive standard deviation. (hint: you'll have to change the loss and generate random data for regression!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All models are callable, just like layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Input(shape=(784,))\n",
    "# this works, and returns the 10-way softmax we defined above.\n",
    "y = model1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed\n",
    "\n",
    "# input tensor for sequences of 20 timesteps,\n",
    "# each containing a 784-dimensional vector\n",
    "input_sequences = Input(shape=(20, 784))\n",
    "\n",
    "# this applies our previous model to every timestep in the input sequences.\n",
    "# the output of the previous model was a 10-way softmax,\n",
    "# so the output of the layer below will be a sequence of 20 vectors of size 10.\n",
    "processed_sequences = TimeDistributed(model1)(input_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-input and multi-output models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense, merge\n",
    "from keras.models import Model\n",
    "\n",
    "# headline input: meant to receive sequences of 100 integers, between 1 and 10000.\n",
    "# note that we can name any layer by passing it a \"name\" argument.\n",
    "main_input = Input(shape=(100,), dtype='int32', name='main_input')\n",
    "\n",
    "# this embedding layer will encode the input sequence\n",
    "# into a sequence of dense 512-dimensional vectors.\n",
    "x = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input)\n",
    "\n",
    "# an LSTM will transform the vector sequence into a single vector,\n",
    "# containing information about the entire sequence\n",
    "lstm_out = LSTM(32)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auxiliary_output = Dense(1, activation='sigmoid', name='aux_output')(lstm_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auxiliary_input = Input(shape=(5,), name='aux_input')\n",
    "x = merge([lstm_out, auxiliary_input], mode='concat')\n",
    "\n",
    "# we stack a deep fully-connected network on top\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "# and finally we add the main logistic regression layer\n",
    "main_output = Dense(1, activation='sigmoid', name='main_output')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = Model(input=[main_input, auxiliary_input], output=[main_output, auxiliary_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model2.compile(optimizer='rmsprop',\n",
    "               loss={'main_output': 'binary_crossentropy', 'aux_output': 'binary_crossentropy'},\n",
    "               loss_weights={'main_output': 1., 'aux_output': 0.2})\n",
    "\n",
    "# and trained it via:\n",
    "model2.fit({'main_input': headline_data, 'aux_input': additional_data},\n",
    "           {'main_output': labels2, 'aux_output': labels2},\n",
    "           nb_epoch=5, verbose=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Challenge 2</font>\n",
    "Adapt the code above, preceding the LSTM with model1 applied to each input symbol in the sequence (hint: you'll have to adapt Embedding as well). When done, add dropout to the LSTM (see https://keras.io/layers/recurrent/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Dense, merge\n",
    "from keras.models import Model\n",
    "\n",
    "tweet_a = Input(shape=(140, 256))\n",
    "tweet_b = Input(shape=(140, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this layer can take as input a matrix\n",
    "# and will return a vector of size 64\n",
    "shared_lstm = LSTM(64)\n",
    "\n",
    "# when we reuse the same layer instance\n",
    "# multiple times, the weights of the layer\n",
    "# are also being reused\n",
    "# (it is effectively *the same* layer)\n",
    "encoded_a = shared_lstm(tweet_a)\n",
    "encoded_b = shared_lstm(tweet_b)\n",
    "\n",
    "# we can then concatenate the two vectors:\n",
    "merged_vector = merge([encoded_a, encoded_b], mode='concat', concat_axis=-1)\n",
    "\n",
    "# and add a logistic regression on top\n",
    "predictions = Dense(1, activation='sigmoid')(merged_vector)\n",
    "\n",
    "# we define a trainable model linking the\n",
    "# tweet inputs to the predictions\n",
    "model3 = Model(input=[tweet_a, tweet_b], output=predictions)\n",
    "\n",
    "model3.compile(optimizer='rmsprop',\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])\n",
    "model3.fit([data_a, data_b], labels2, nb_epoch=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Challenge 3</font>\n",
    "Adapt the code above to generate an answer given an MNIST image and a question about it (using model1, restricting the vocabulary to 1000 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some more examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception module\n",
    "https://arxiv.org/abs/1409.4842"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import merge, Convolution2D, MaxPooling2D, Input\n",
    "\n",
    "input_img = Input(shape=(3, 256, 256))\n",
    "\n",
    "tower_1 = Convolution2D(64, 1, 1, border_mode='same', activation='relu')(input_img)\n",
    "tower_1 = Convolution2D(64, 3, 3, border_mode='same', activation='relu')(tower_1)\n",
    "\n",
    "tower_2 = Convolution2D(64, 1, 1, border_mode='same', activation='relu')(input_img)\n",
    "tower_2 = Convolution2D(64, 5, 5, border_mode='same', activation='relu')(tower_2)\n",
    "\n",
    "tower_3 = MaxPooling2D((3, 3), strides=(1, 1), border_mode='same')(input_img)\n",
    "tower_3 = Convolution2D(64, 1, 1, border_mode='same', activation='relu')(tower_3)\n",
    "\n",
    "output = merge([tower_1, tower_2, tower_3], mode='concat', concat_axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual connection on a convolution layer\n",
    "https://arxiv.org/abs/1512.03385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import merge, Convolution2D, Input\n",
    "\n",
    "# input tensor for a 3-channel 256x256 image\n",
    "x = Input(shape=(3, 256, 256))\n",
    "# 3x3 conv with 3 output channels (same as input channels)\n",
    "y = Convolution2D(3, 3, 3, border_mode='same')(x)\n",
    "# this returns x + y.\n",
    "z = merge([x, y], mode='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared vision model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import merge, Convolution2D, MaxPooling2D, Input, Dense, Flatten\n",
    "from keras.models import Model\n",
    "\n",
    "# first, define the vision modules\n",
    "digit_input = Input(shape=(1, 27, 27))\n",
    "x = Convolution2D(64, 3, 3)(digit_input)\n",
    "x = Convolution2D(64, 3, 3)(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "out = Flatten()(x)\n",
    "\n",
    "vision_model = Model(digit_input, out)\n",
    "\n",
    "# then define the tell-digits-apart model\n",
    "digit_a = Input(shape=(1, 27, 27))\n",
    "digit_b = Input(shape=(1, 27, 27))\n",
    "\n",
    "# the vision model will be shared, weights and all\n",
    "out_a = vision_model(digit_a)\n",
    "out_b = vision_model(digit_b)\n",
    "\n",
    "concatenated = merge([out_a, out_b], mode='concat')\n",
    "out = Dense(1, activation='sigmoid')(concatenated)\n",
    "\n",
    "classification_model = Model([digit_a, digit_b], out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual question answering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Convolution2D, MaxPooling2D, Flatten\n",
    "from keras.layers import Input, LSTM, Embedding, Dense, merge\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "# first, let's define a vision model using a Sequential model.\n",
    "# this model will encode an image into a vector.\n",
    "vision_model = Sequential()\n",
    "vision_model.add(Convolution2D(64, 3, 3, activation='relu', border_mode='same', input_shape=(3, 224, 224)))\n",
    "vision_model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "vision_model.add(MaxPooling2D((2, 2)))\n",
    "vision_model.add(Convolution2D(128, 3, 3, activation='relu', border_mode='same'))\n",
    "vision_model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "vision_model.add(MaxPooling2D((2, 2)))\n",
    "vision_model.add(Convolution2D(256, 3, 3, activation='relu', border_mode='same'))\n",
    "vision_model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "vision_model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "vision_model.add(MaxPooling2D((2, 2)))\n",
    "vision_model.add(Flatten())\n",
    "\n",
    "# now let's get a tensor with the output of our vision model:\n",
    "image_input = Input(shape=(3, 224, 224))\n",
    "encoded_image = vision_model(image_input)\n",
    "\n",
    "# next, let's define a language model to encode the question into a vector.\n",
    "# each question will be at most 100 word long,\n",
    "# and we will index words as integers from 1 to 9999.\n",
    "question_input = Input(shape=(100,), dtype='int32')\n",
    "embedded_question = Embedding(input_dim=10000, output_dim=256, input_length=100)(question_input)\n",
    "encoded_question = LSTM(256)(embedded_question)\n",
    "\n",
    "# let's concatenate the question vector and the image vector:\n",
    "merged = merge([encoded_question, encoded_image], mode='concat')\n",
    "\n",
    "# and let's train a logistic regression over 1000 words on top:\n",
    "output = Dense(1000, activation='softmax')(merged)\n",
    "\n",
    "# this is our final model:\n",
    "vqa_model = Model(input=[image_input, question_input], output=output)\n",
    "\n",
    "# the next stage would be training this model on actual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video question answering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed\n",
    "\n",
    "video_input = Input(shape=(100, 3, 224, 224))\n",
    "# this is our video encoded via the previously trained vision_model (weights are reused)\n",
    "encoded_frame_sequence = TimeDistributed(vision_model)(video_input)  # the output will be a sequence of vectors\n",
    "encoded_video = LSTM(256)(encoded_frame_sequence)  # the output will be a vector\n",
    "\n",
    "# this is a model-level representation of the question encoder, reusing the same weights as before:\n",
    "question_encoder = Model(input=question_input, output=encoded_question)\n",
    "\n",
    "# let's use it to encode the question:\n",
    "video_question_input = Input(shape=(100,), dtype='int32')\n",
    "encoded_video_question = question_encoder(video_question_input)\n",
    "\n",
    "# and this is our video question answering model:\n",
    "merged = merge([encoded_video, encoded_video_question], mode='concat')\n",
    "output = Dense(1000, activation='softmax')(merged)\n",
    "video_qa_model = Model(input=[video_input, video_question_input], output=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
